{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Подготовка поисковых данных;\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2.Search;\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3.Ask.\n"
      ],
      "metadata": {
        "id": "w3B3lkDKyRNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkHDuJVL7UEK"
      },
      "outputs": [],
      "source": [
        "# Установим библиотеку для работы с Википедией\n",
        "!pip install wikipedia\n",
        "!pip install openai mwclient mwparserfromhell tiktoken\n",
        "import wikipedia\n",
        "\n",
        "# Отключим предупреждения в колабе. Будет меньше лишней информации в выводе\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import mwclient  # библиотека для работы с MediaWiki API для загрузки примеров статей Википедии\n",
        "import mwparserfromhell  # Парсер для MediaWiki\n",
        "import openai  # будем использовать для токинизации\n",
        "import pandas as pd  # В DataFrame будем хранить базу знаний и результат токинизации базы знаний\n",
        "import re  # для вырезания ссылок <ref> из статей Википедии\n",
        "import tiktoken  # для подсчета токенов\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# поиск страниц Википедии о Категории Моря\n",
        "\n",
        "# Задаем категорию и рускоязычную версию Википедии для поиска\n",
        "CATEGORY_TITLE = \"Категория:Моря\"\n",
        "WIKI_SITE = \"ru.wikipedia.org\"\n",
        "\n",
        "# Соберем заголовки всех статей\n",
        "def titles_from_category(\n",
        "    category: mwclient.listing.Category, # Задаем типизированный параметр категории статей\n",
        "    max_depth: int # Определяем глубину вложения статей\n",
        ") -> set[str]:\n",
        "    \"\"\"Возвращает набор заголовков страниц в данной категории Википедии и ее подкатегориях.\"\"\"\n",
        "    titles = set() # Используем множество для хранения заголовков статей\n",
        "    for cm in category.members(): # Перебираем вложенные объекты категории\n",
        "        if type(cm) == mwclient.page.Page: # Если объект является страницей\n",
        "            titles.add(cm.name) # в хранилище заголовков добавляем имя страницы\n",
        "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0: # Если объект является категорией и глубина вложения не достигла максимальной\n",
        "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1) # вызываем рекурсивно функцию для подкатегории\n",
        "            titles.update(deeper_titles) # добавление в множество элементов из другого множества\n",
        "    return titles\n",
        "\n",
        "# Инициализация объекта MediaWiki\n",
        "# WIKI_SITE ссылается на рускоязычную часть Википедии\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "\n",
        "# Загрузка раздела заданной категории\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "# Получение множества всех заголовков категории с вложенностью на один уровень\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "\n",
        "\n",
        "print(f\"Создано {len(titles)} заголовков статей в категории {CATEGORY_TITLE}.\")\n",
        "\n",
        "# Задаем секции, которые будут отброшены при парсинге статей\n",
        "SECTIONS_TO_IGNORE = [\n",
        "    \"Сноски\",\n",
        "    \"Библиография\",\n",
        "    \"Источники\",\n",
        "    \"Цитаты\",\n",
        "    \"Литература\",\n",
        "    \"Примечания и ссылки\",\n",
        "    \"Фотогалерея\",\n",
        "    \"Цитируемые работы\",\n",
        "    \"Фотографии\",\n",
        "    \"Галерея\",\n",
        "    \"Примечания\",\n",
        "    \"Ссылки и источники\",\n",
        "    \"Ссылки и примечания\",\n",
        "]\n",
        "\n",
        "# Функция возвращает список всех вложенных секций для заданной секции страницы Википедии\n",
        "\n",
        "def all_subsections_from_section(\n",
        "    section: mwparserfromhell.wikicode.Wikicode, # текущая секция\n",
        "    parent_titles: list[str], # Заголовки родителя\n",
        "    sections_to_ignore: set[str], # Секции, которые необходимо проигнорировать\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    Из раздела Википедии возвращает список всех вложенных секций.\n",
        "    Каждый подраздел представляет собой кортеж, где:\n",
        "      - первый элемент представляет собой список родительских секций, начиная с заголовка страницы\n",
        "      - второй элемент представляет собой текст секции\n",
        "    \"\"\"\n",
        "\n",
        "    # Извлекаем заголовки текущей секции\n",
        "    headings = [str(h) for h in section.filter_headings()]\n",
        "    title = headings[0]\n",
        "    # Заголовки Википедии имеют вид: \"== Heading ==\"\n",
        "\n",
        "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
        "        # Если заголовок секции в списке для игнора, то пропускаем его\n",
        "        return []\n",
        "\n",
        "    # Объединим заголовки и подзаголовки, чтобы сохранить контекст для chatGPT\n",
        "    titles = parent_titles + [title]\n",
        "\n",
        "    # Преобразуем wikicode секции в строку\n",
        "    full_text = str(section)\n",
        "\n",
        "    # Выделяем текст секции без заголовка\n",
        "    section_text = full_text.split(title)[1]\n",
        "    if len(headings) == 1:\n",
        "        # Если один заголовок, то формируем результирующий список\n",
        "        return [(titles, section_text)]\n",
        "    else:\n",
        "        first_subtitle = headings[1]\n",
        "        section_text = section_text.split(first_subtitle)[0]\n",
        "        # Формируем результирующий список из текста до первого подзаголовка\n",
        "        results = [(titles, section_text)]\n",
        "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
        "            results.extend(\n",
        "                # Вызываем функцию получения вложенных секций для заданной секции\n",
        "                all_subsections_from_section(subsection, titles, sections_to_ignore)\n",
        "                )  # Объединяем результирующие списки данной функции и вызываемой\n",
        "        return results\n",
        "\n",
        "# Функция возвращает список всех секций страницы, за исключением тех, которые отбрасываем\n",
        "def all_subsections_from_title(\n",
        "    title: str, # Заголовок статьи Википедии, которую парсим\n",
        "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE, # Секции, которые игнорируем\n",
        "    site_name: str = WIKI_SITE, # Ссылка на сайт википедии\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    Из заголовка страницы Википедии возвращает список всех вложенных секций.\n",
        "    Каждый подраздел представляет собой кортеж, где:\n",
        "      - первый элемент представляет собой список родительских секций, начиная с заголовка страницы\n",
        "      - второй элемент представляет собой текст секции\n",
        "    \"\"\"\n",
        "\n",
        "    # Инициализация объекта MediaWiki\n",
        "    # WIKI_SITE ссылается на рускоязычную часть Википедии\n",
        "    site = mwclient.Site(site_name)\n",
        "\n",
        "    # Запрашиваем страницу по заголовку\n",
        "    page = site.pages[title]\n",
        "\n",
        "    # Получаем текстовое представление страницы\n",
        "    text = page.text()\n",
        "\n",
        "    # Удобный парсер для MediaWiki\n",
        "    parsed_text = mwparserfromhell.parse(text)\n",
        "    # Извлекаем заголовки\n",
        "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "    if headings: # Если заголовки найдены\n",
        "        # В качестве резюме берем текст до первого заголовка\n",
        "        summary_text = str(parsed_text).split(headings[0])[0]\n",
        "    else:\n",
        "        # Если нет заголовков, то весь текст считаем резюме\n",
        "        summary_text = str(parsed_text)\n",
        "    results = [([title], summary_text)] # Добавляем резюме в результирующий список\n",
        "    for subsection in parsed_text.get_sections(levels=[2]): # Извлекаем секции 2-го уровня\n",
        "        results.extend(\n",
        "            # Вызываем функцию получения вложенных секций для заданной секции\n",
        "            all_subsections_from_section(subsection, [title], sections_to_ignore)\n",
        "        ) # Объединяем результирующие списки данной функции и вызываемой\n",
        "    return results\n",
        "\n",
        "  # Разбивка статей на секции\n",
        "# придется немного подождать, так как на парсинг 100 статей требуется около минуты\n",
        "wikipedia_sections = []\n",
        "for title in titles:\n",
        "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
        "print(f\"Найдено {len(wikipedia_sections)} секций на {len(titles)} страницах\")\n",
        "\n",
        "# Очистка текста секции от ссылок <ref>xyz</ref>, начальных и конечных пробелов\n",
        "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
        "    titles, text = section\n",
        "    # Удаляем ссылки\n",
        "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
        "    # Удаляем пробелы вначале и конце\n",
        "    text = text.strip()\n",
        "    return (titles, text)\n",
        "\n",
        "# Применим функцию очистки ко всем секциям с помощью генератора списков\n",
        "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
        "\n",
        "# Отфильтруем короткие и пустые секции\n",
        "def keep_section(section: tuple[list[str], str]) -> bool:\n",
        "    \"\"\"Возвращает значение True, если раздел должен быть сохранен, в противном случае значение False.\"\"\"\n",
        "    titles, text = section\n",
        "    # Фильтруем по произвольной длине, можно выбрать и другое значение\n",
        "    if len(text) < 16:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "original_num_sections = len(wikipedia_sections)\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
        "print(f\"Отфильтровано {original_num_sections-len(wikipedia_sections)} секций, осталось {len(wikipedia_sections)} секций.\")\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "# Функция подсчета токенов\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция разделения строк\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Разделяет строку надвое с помощью разделителя (delimiter), пытаясь сбалансировать токены с каждой стороны.\"\"\"\n",
        "\n",
        "    # Делим строку на части по разделителю, по умолчанию \\n - перенос строки\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # разделитель не найден\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # нет необходимости искать промежуточную точку\n",
        "    else:\n",
        "        # Считаем токены\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        # Предварительное разделение по середине числа токенов\n",
        "        best_diff = halfway\n",
        "        # В цикле ищем какой из разделителей, будет ближе всего к best_diff\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        # Возвращаем левую и правую часть оптимально разделенной строки\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "# Функция обрезает строку до максимально разрешенного числа токенов\n",
        "def truncated_string(\n",
        "    string: str, # строка\n",
        "    model: str, # модель\n",
        "    max_tokens: int, # максимальное число разрешенных токенов\n",
        "    print_warning: bool = True, # флаг вывода предупреждения\n",
        ") -> str:\n",
        "    \"\"\"Обрезка строки до максимально разрешенного числа токенов.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    # Обрезаем строку и декодируем обратно\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Предупреждение: Строка обрезана с {len(encoded_string)} токенов до {max_tokens} токенов.\")\n",
        "    # Усеченная строка\n",
        "    return truncated_string\n",
        "\n",
        "# Функция делит секции статьи на части по максимальному числу токенов\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str], # секции\n",
        "    max_tokens: int = 1000, # максимальное число токенов\n",
        "    model: str = GPT_MODEL, # модель\n",
        "    max_recursion: int = 5, # максимальное число рекурсий\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Разделяет секции на список из частей секций, в каждой части не более max_tokens.\n",
        "    Каждая часть представляет собой кортеж родительских заголовков [H1, H2, ...] и текста (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # Если длина соответствует допустимой, то вернет строку\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # если в результате рекурсия не удалось разделить строку, то просто усечем ее по числу токенов\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # иначе разделим пополам и выполним рекурсию\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]: # Пробуем использовать разделители от большего к меньшему (разрыв, абзац, точка)\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # если какая-либо половина пуста, повторяем попытку с более простым разделителем\n",
        "                continue\n",
        "            else:\n",
        "                # применим рекурсию на каждой половине\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1, # уменьшаем максимальное число рекурсий\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # иначе никакого разделения найдено не было, поэтому просто обрезаем строку (должно быть очень редко)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "\n",
        "    # Делим секции на части\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} секций Википедии поделены на {len(wikipedia_strings)} строк.\")\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # Модель токенизации от OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")\n",
        "client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Функция отправки chatGPT строки для ее токенизации (вычисления эмбедингов)\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "df = pd.DataFrame({\"text\": wikipedia_strings[:10]})\n",
        "\n",
        "df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
        "\n",
        "SAVE_PATH = \"./sea.csv\"\n",
        "# Сохранение результата\n",
        "df.to_csv(SAVE_PATH, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ask"
      ],
      "metadata": {
        "id": "D94_nZ29-NJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from scipy import spatial  # вычисляет сходство векторов\n",
        "embeddings_path = \"/content/sea.csv\"\n",
        "\n",
        "df = pd.read_csv(embeddings_path)\n",
        "\n",
        "# Конвертируем наши эмбединги из строк в списки\n",
        "df['embedding'] = df['embedding'].apply(ast.literal_eval)\n",
        "\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Функция поиска\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str, # пользовательский запрос\n",
        "    df: pd.DataFrame, # DataFrame со столбцами text и embedding (база знаний)\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), # функция схожести, косинусное расстояние\n",
        "    top_n: int = 100 # выбор лучших n-результатов\n",
        ") -> tuple[list[str], list[float]]: # Функция возвращает кортеж двух списков, первый содержит строки, второй - числа с плавающей запятой\n",
        "    \"\"\"Возвращает строки и схожести, отсортированные от большего к меньшему\"\"\"\n",
        "\n",
        "    # Отправляем в OpenAI API пользовательский запрос для токенизации\n",
        "    query_embedding_response = openai.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query,\n",
        "    )\n",
        "\n",
        "    # Получен токенизированный пользовательский запрос\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "\n",
        "    # Сравниваем пользовательский запрос с каждой токенизированной строкой DataFrame\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "    # Сортируем по убыванию схожести полученный список\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Преобразовываем наш список в кортеж из списков\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "\n",
        "    # Возвращаем n лучших результатов\n",
        "    return strings[:top_n], relatednesses[:top_n]\n",
        "\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке для заданной модели\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция формирования запроса к chatGPT по пользовательскому вопросу и базе знаний\n",
        "def query_message(\n",
        "    query: str, # пользовательский запрос\n",
        "    df: pd.DataFrame, # DataFrame со столбцами text и embedding (база знаний)\n",
        "    model: str, # модель\n",
        "    token_budget: int # ограничение на число отсылаемых токенов в модель\n",
        ") -> str:\n",
        "    \"\"\"Возвращает сообщение для GPT с соответствующими исходными текстами, извлеченными из фрейма данных (базы знаний).\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df) # функция ранжирования базы знаний по пользовательскому запросу\n",
        "    # Шаблон инструкции для chatGPT\n",
        "    message = 'Используйте приведенные ниже статьи о морях, чтобы ответить на следующий вопрос. Если ответ не может быть найден в статьях, напишите \"Я не смог найти ответ.\"'\n",
        "    # Шаблон для вопроса\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "\n",
        "    # Добавляем к сообщению для chatGPT релевантные строки из базы знаний, пока не выйдем за допустимое число токенов\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (num_tokens(message + next_article + question, model=model) > token_budget):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(\n",
        "    query: str, # пользовательский запрос\n",
        "    df: pd.DataFrame = df, # DataFrame со столбцами text и embedding (база знаний)\n",
        "    model: str = GPT_MODEL, # модель\n",
        "    token_budget: int = 4096 - 500, # ограничение на число отсылаемых токенов в модель\n",
        "    print_message: bool = False, # нужно ли выводить сообщение перед отправкой\n",
        ") -> str:\n",
        "    \"\"\"Отвечает на вопрос, используя GPT и базу знаний.\"\"\"\n",
        "    # Формируем сообщение к chatGPT (функция выше)\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    # Если параметр True, то выводим сообщение\n",
        "    if print_message:\n",
        "        print(message)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Вы отвечаете на вопросы о морях. О глубине, о происхождении, истории. Кем было открыто.\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0 # гиперпараметр степени случайности при генерации текста. Влияет на то, как модель выбирает следующее слово в последовательности.\n",
        "    )\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message\n",
        "\n",
        "ask(\"Самая глубокая точка в море\")\n"
      ],
      "metadata": {
        "id": "yUIhAPjw-Mvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация TGBota"
      ],
      "metadata": {
        "id": "boNhDpB0D1PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "!pip install aiogram\n",
        "import aiogram\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters.command import Command\n",
        "import asyncio\n",
        "API_TOKEN = ''  # Замените на свой реальный токен Telegram Bot API\n",
        "bot = Bot(token=API_TOKEN)\n",
        "dp = Dispatcher()\n",
        "\n",
        "\n",
        "# Хэндлер на команду /help\n",
        "@dp.message(Command(\"help\"))\n",
        "async def cmd_help(message: types.Message):\n",
        "    # Предоставьте информацию о базе знаний\n",
        "    command = (\n",
        "        \"Тематика: Темы о морях\\n\"\n",
        "        f\"Число записей в базе знаний: {len(df)}\\n\"\n",
        "        \"Пример запроса к базе: /query\"\n",
        "    )\n",
        "\n",
        "    # Отправьте информацию о базе знаний пользователю\n",
        "    await message.answer(command)\n",
        "\n",
        "@dp.message(Command(\"query\"))\n",
        "async def process_query(message: types.Message):\n",
        "    # Извлеките запрос пользователя из текста сообщения\n",
        "    query = message.text.replace('/query', '').strip()\n",
        "\n",
        "    # Используйте функцию ask для получения ответа\n",
        "    response = ask(query, print_message=False)\n",
        "\n",
        "    # Отправьте ответ пользователю\n",
        "    await message.answer(response)\n",
        "\n",
        "@dp.message()\n",
        "async def process_all_messages(message: types.Message):\n",
        "    # Извлеките запрос пользователя из текста сообщения\n",
        "    query = message.text.strip()\n",
        "\n",
        "    # Используйте функцию ask для получения ответа\n",
        "    response = ask(query, print_message=False)\n",
        "\n",
        "    # Отправьте ответ пользователю\n",
        "    await message.answer(response)\n",
        "\n",
        "\n",
        "# Запуск процесса поллинга новых апдейтов\n",
        "async def main():\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "# Запустите бота\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "yz9GibKZD3Or"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
